{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a model with a sample from the MNIST data such that only 3 & 7 numbers get identified.\n",
    "Fast AI can train this in a one line, but idea is to learn SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    " # All imports\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.14% [3219456/3214948 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All preparation work\n",
    "# Read data from the path\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "Path.BASE_PATH = path\n",
    "threes = (path/'train/3').ls().sorted() #6131 images of 3\n",
    "sevens = (path/'train/7').ls().sorted() # 6265 images of 7, total = 12396 of training data\n",
    "valid = (path/'valid').ls().sorted()\n",
    "tensor_threes = [tensor(Image.open(i)) for i in threes]\n",
    "tensor_sevens = [tensor(Image.open(i)) for i in sevens]\n",
    "\n",
    "# For vision related work, convert all pixel data in range 0 -1\n",
    "stacked_tensors_threes = torch.stack(tensor_threes).float() / 255.0    #[6131, 28, 28]\n",
    "stacked_tensors_sevens = torch.stack(tensor_sevens).float() / 255.0    #[6131, 28, 28]\n",
    "\n",
    "# Do same for validation data, read data, convert pixel data 0-1, \n",
    "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float()/255.0 \n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\n",
    "valid_7_tens = valid_7_tens.float()/255.0\n",
    "\n",
    "# Prepare x,y for tensor usage -> here each row is one image with 18*28 columns\n",
    "\n",
    "# Convert a rank 3 tensor to a rank 2 tensor, -1 is a special parameter to view that \n",
    "#means \"make this axis as big as necessary to fit all the data\":\n",
    "train_x = torch.cat([stacked_tensors_threes, stacked_tensors_sevens]).view(-1, 28*28)  # [12396, 784]\n",
    "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\n",
    "\n",
    "# Create lables for training & validation set\n",
    "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)                       # [12396, 1]\n",
    "valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
    "\n",
    "# Dataset in Pytorch is required a return a tuple (x,y) when indexed. zip() with a list is used to achieve this\n",
    "dset = list(zip(train_x,train_y)) # Each element of dset is a (tensor[784], tensor[1])\n",
    "valid_dset = list(zip(valid_x,valid_y))\n",
    "\n",
    "# Here we will use Dataloader object to run mini batches, thus helping it to run in the form of SGD\n",
    "dl = DataLoader(dset, batch_size=256)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Functions\n",
    "# Using Sigmoid function to calculate the loss\n",
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
    "\n",
    "# Calculating the gradient, takes the data, feed it in a model to get a prediction\n",
    "# Calculate the loss using the Sigmoid function\n",
    "# Do the backpropagation to find the gradient\n",
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()\n",
    "\n",
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7185 0.8515 0.9135 0.9369 0.9505 0.9608 0.9657 0.9667 0.9691 0.9701 0.9706 0.9706 0.9716 0.9721 0.974 0.9731 0.974 0.9745 0.976 0.9765 "
     ]
    }
   ],
   "source": [
    "######################################## RAW Implementation of SGD ####################################################\n",
    "#######################################################################################################\n",
    "# Here it takes the data from the data loader to calculate the gradients\n",
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n",
    "            \n",
    "# Train the model over some epochs \n",
    "def train_model(model, epochs, lr):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model, lr)\n",
    "        print(validate_epoch(model), end=' ')\n",
    "        \n",
    "# Sample \n",
    "def linear1(xb): \n",
    "    return xb@weights + bias\n",
    "\n",
    "# Parameters initialisation\n",
    "def init_params(size, std=1.0): \n",
    "    return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "lr = 1.0\n",
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)\n",
    "params = weights,bias\n",
    "\n",
    "for i in range(20):\n",
    "    train_epoch(linear1, lr, params)\n",
    "    print(validate_epoch(linear1), end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An optimiser can be something like this\n",
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None\n",
    "\n",
    "linear_model = nn.Linear(28*28,1)   \n",
    "opt = BasicOptim(linear_model.parameters(), lr)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.8481 0.8281 0.9106 0.934 0.9472 0.9555 0.9618 0.9653 0.9682 0.9692 0.9716 0.9741 0.975 0.976 0.977 0.9775 0.978 0.9785 0.9785 "
     ]
    }
   ],
   "source": [
    "############################################ SGD using pytorch Library Calls ############################################################\n",
    "###########################################################################################################################\n",
    "# opt.step() is the gradient descnet step where it updates the parameter = gradient * lr. SGD takes care of this\n",
    "# loss.backward() in calc_grad() calculates the gradients and add them to the existing one\n",
    "# So zero_grad() is called to update the gradient value to 0, where it sets the weight & bias to 0 \n",
    "def train_epoch_sgd(model):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "def train_model_sgd(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch_sgd(model)\n",
    "        print(validate_epoch(model), end=' ')\n",
    "        \n",
    "\n",
    "#Using SGD         \n",
    "lr = 1.0\n",
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model_sgd(linear_model, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train by Fast AI Library Function Learner() which warps the train_epoch() and train_model() functions. DataLoaders is a FASTAI class which takes multiple pytorch dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.636839</td>\n",
       "      <td>0.503688</td>\n",
       "      <td>0.495584</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.626936</td>\n",
       "      <td>0.130309</td>\n",
       "      <td>0.912659</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.226317</td>\n",
       "      <td>0.283088</td>\n",
       "      <td>0.723258</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.097773</td>\n",
       "      <td>0.135289</td>\n",
       "      <td>0.882237</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.050047</td>\n",
       "      <td>0.090611</td>\n",
       "      <td>0.921001</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.031451</td>\n",
       "      <td>0.069645</td>\n",
       "      <td>0.939647</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023786</td>\n",
       "      <td>0.057385</td>\n",
       "      <td>0.951914</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.049479</td>\n",
       "      <td>0.960255</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018734</td>\n",
       "      <td>0.044078</td>\n",
       "      <td>0.964671</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>0.040197</td>\n",
       "      <td>0.966143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = DataLoaders(dl, valid_dl)\n",
    "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "learn.fit(10, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train by Fast AI Optimised Function - One liner - better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.143675</td>\n",
       "      <td>0.019299</td>\n",
       "      <td>0.997056</td>\n",
       "      <td>07:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = ImageDataLoaders.from_folder(path)\n",
    "learn = cnn_learner(dls, resnet18, pretrained=False,\n",
    "                    loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
