{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a model with a sample from the MNIST data such that only 3 & 7 numbers get identified.\n",
    "Fast AI can train this in a one line, but idea is to learn SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    " # All imports\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.14% [3219456/3214948 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All preparation work\n",
    "# Read data from the path\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "Path.BASE_PATH = path\n",
    "threes = (path/'train/3').ls().sorted() #6131 images of 3\n",
    "sevens = (path/'train/7').ls().sorted() # 6265 images of 7, total = 12396 of training data\n",
    "valid = (path/'valid').ls().sorted()\n",
    "tensor_threes = [tensor(Image.open(i)) for i in threes]\n",
    "tensor_sevens = [tensor(Image.open(i)) for i in sevens]\n",
    "\n",
    "# For vision related work, convert all pixel data in range 0 -1\n",
    "stacked_tensors_threes = torch.stack(tensor_threes).float() / 255.0    #[6131, 28, 28]\n",
    "stacked_tensors_sevens = torch.stack(tensor_sevens).float() / 255.0    #[6131, 28, 28]\n",
    "\n",
    "# Do same for validation data, read data, convert pixel data 0-1, \n",
    "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float()/255.0 \n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\n",
    "valid_7_tens = valid_7_tens.float()/255.0\n",
    "\n",
    "# Prepare x,y for tensor usage -> here each row is one image with 18*28 columns\n",
    "\n",
    "# Convert a rank 3 tensor to a rank 2 tensor, -1 is a special parameter to view that \n",
    "#means \"make this axis as big as necessary to fit all the data\":\n",
    "train_x = torch.cat([stacked_tensors_threes, stacked_tensors_sevens]).view(-1, 28*28)  # [12396, 784]\n",
    "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\n",
    "\n",
    "# Create lables for training & validation set\n",
    "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)                       # [12396, 1]\n",
    "valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
    "\n",
    "# Dataset in Pytorch is required a return a tuple (x,y) when indexed. zip() with a list is used to achieve this\n",
    "dset = list(zip(train_x,train_y)) # Each element of dset is a (tensor[784], tensor[1])\n",
    "valid_dset = list(zip(valid_x,valid_y))\n",
    "\n",
    "# Here we will use Dataloader object to run mini batches, thus helping it to run in the form of SGD\n",
    "dl = DataLoader(dset, batch_size=256)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Functions\n",
    "# Using Sigmoid function to calculate the loss\n",
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
    "\n",
    "# Calculating the gradient, takes the data, feed it in a model to get a prediction\n",
    "# Calculate the loss using the Sigmoid function\n",
    "# Do the backpropagation to find the gradient\n",
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()\n",
    "\n",
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7185 0.8515 0.9135 0.9369 0.9505 0.9608 0.9657 0.9667 0.9691 0.9701 0.9706 0.9706 0.9716 0.9721 0.974 0.9731 0.974 0.9745 0.976 0.9765 "
     ]
    }
   ],
   "source": [
    "######################################## RAW Implementation of SGD ####################################################\n",
    "#######################################################################################################\n",
    "# Here it takes the data from the data loader to calculate the gradients\n",
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr\n",
    "            p.grad.zero_()\n",
    "            \n",
    "# Train the model over some epochs \n",
    "def train_model(model, epochs, lr):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model, lr)\n",
    "        print(validate_epoch(model), end=' ')\n",
    "        \n",
    "# Sample \n",
    "def linear1(xb): \n",
    "    return xb@weights + bias\n",
    "\n",
    "# Parameters initialisation\n",
    "def init_params(size, std=1.0): \n",
    "    return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "lr = 1.0\n",
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)\n",
    "params = weights,bias\n",
    "\n",
    "for i in range(20):\n",
    "    train_epoch(linear1, lr, params)\n",
    "    print(validate_epoch(linear1), end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An optimiser can be something like this\n",
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None\n",
    "\n",
    "linear_model = nn.Linear(28*28,1)   \n",
    "opt = BasicOptim(linear_model.parameters(), lr)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.8481 0.8281 0.9106 0.934 0.9472 0.9555 0.9618 0.9653 0.9682 0.9692 0.9716 0.9741 0.975 0.976 0.977 0.9775 0.978 0.9785 0.9785 "
     ]
    }
   ],
   "source": [
    "############################################ SGD using pytorch Library Calls ############################################################\n",
    "###########################################################################################################################\n",
    "# opt.step() is the gradient descnet step where it updates the parameter = gradient * lr. SGD takes care of this\n",
    "# loss.backward() in calc_grad() calculates the gradients and add them to the existing one\n",
    "# So zero_grad() is called to update the gradient value to 0, where it sets the weight & bias to 0 \n",
    "def train_epoch_sgd(model):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "def train_model_sgd(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch_sgd(model)\n",
    "        print(validate_epoch(model), end=' ')\n",
    "        \n",
    "\n",
    "#Using SGD         \n",
    "lr = 1.0\n",
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model_sgd(linear_model, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train by Fast AI Library Function Learner() which warps the train_epoch() and train_model() functions. DataLoaders is a FASTAI class which takes multiple pytorch dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.636839</td>\n",
       "      <td>0.503688</td>\n",
       "      <td>0.495584</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.626936</td>\n",
       "      <td>0.130309</td>\n",
       "      <td>0.912659</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.226317</td>\n",
       "      <td>0.283088</td>\n",
       "      <td>0.723258</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.097773</td>\n",
       "      <td>0.135289</td>\n",
       "      <td>0.882237</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.050047</td>\n",
       "      <td>0.090611</td>\n",
       "      <td>0.921001</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.031451</td>\n",
       "      <td>0.069645</td>\n",
       "      <td>0.939647</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023786</td>\n",
       "      <td>0.057385</td>\n",
       "      <td>0.951914</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.049479</td>\n",
       "      <td>0.960255</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018734</td>\n",
       "      <td>0.044078</td>\n",
       "      <td>0.964671</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>0.040197</td>\n",
       "      <td>0.966143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = DataLoaders(dl, valid_dl)\n",
    "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "learn.fit(10, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.320865</td>\n",
       "      <td>0.424981</td>\n",
       "      <td>0.504416</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.150803</td>\n",
       "      <td>0.237268</td>\n",
       "      <td>0.794897</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.082852</td>\n",
       "      <td>0.117884</td>\n",
       "      <td>0.914132</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.054063</td>\n",
       "      <td>0.078922</td>\n",
       "      <td>0.939156</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040772</td>\n",
       "      <td>0.061328</td>\n",
       "      <td>0.956330</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034008</td>\n",
       "      <td>0.051554</td>\n",
       "      <td>0.963690</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030142</td>\n",
       "      <td>0.045433</td>\n",
       "      <td>0.965162</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027643</td>\n",
       "      <td>0.041266</td>\n",
       "      <td>0.966143</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025850</td>\n",
       "      <td>0.038241</td>\n",
       "      <td>0.968597</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024464</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.970559</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>0.034108</td>\n",
       "      <td>0.972031</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022403</td>\n",
       "      <td>0.032613</td>\n",
       "      <td>0.973013</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021604</td>\n",
       "      <td>0.031358</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020915</td>\n",
       "      <td>0.030282</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020311</td>\n",
       "      <td>0.029345</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019777</td>\n",
       "      <td>0.028518</td>\n",
       "      <td>0.976448</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019299</td>\n",
       "      <td>0.027783</td>\n",
       "      <td>0.977920</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018869</td>\n",
       "      <td>0.027122</td>\n",
       "      <td>0.977920</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.018478</td>\n",
       "      <td>0.026527</td>\n",
       "      <td>0.977920</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.018121</td>\n",
       "      <td>0.025986</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017792</td>\n",
       "      <td>0.025494</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.017488</td>\n",
       "      <td>0.025043</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.017206</td>\n",
       "      <td>0.024628</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.016942</td>\n",
       "      <td>0.024246</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>0.023892</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016463</td>\n",
       "      <td>0.023564</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.016244</td>\n",
       "      <td>0.023260</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.016038</td>\n",
       "      <td>0.022977</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.015842</td>\n",
       "      <td>0.022713</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.015657</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.015481</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.015313</td>\n",
       "      <td>0.022020</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.015153</td>\n",
       "      <td>0.021817</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.021627</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.014854</td>\n",
       "      <td>0.021448</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.014713</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.014578</td>\n",
       "      <td>0.021121</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.014449</td>\n",
       "      <td>0.020971</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>0.020830</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.014203</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.983316957950592"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using non linearity, Using neural network\n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")\n",
    "dls = DataLoaders(dl, valid_dl)\n",
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "learn.fit(40, 0.1)\n",
    "# Plots the accuracy of the model\n",
    "plt.plot(L(learn.recorder.values).itemgot(2));\n",
    "learn.recorder.values[-1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train by Fast AI Optimised Function - One liner - better performance with only one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.143675</td>\n",
       "      <td>0.019299</td>\n",
       "      <td>0.997056</td>\n",
       "      <td>07:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = ImageDataLoaders.from_folder(path)\n",
    "learn = cnn_learner(dls, resnet18, pretrained=False,\n",
    "                    loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
